---
title: "HLM for all TST - Template"
author: "Clara Belitz"
date: "04/13/2023"
output: html_document
---


```{r setup, include=FALSE}
library(lme4)      # for fitting model
library(lmerTest)  # for getting df, t and p for fixed effects
library(texreg)    # nice formated output (laTex included)
library(optimx)    # needed for changing algorithm
```

## Variables chosen for this output
```{r variables}
# Set variables for usage in the file. Here 'model_col' refers to the column name for that model
file_name = "path/to/filename.csv" # not currently working, need to set manually in next chunk

# rf_col = 'rf_pred_thresh'
# dt_col = 'dt_pred_thresh'
# nn_col = 'nn_pred_thresh'
# xgb_col = 'xgb_pred_thresh'

rf_col = 'rf_label_predict'
dt_col = 'dt_label_predict'
nn_col = 'nn_label_predict'
xgb_col = 'xgb_label_predict'

```

## Data Setup

We need to load the general training data, make student ID and the demographic group of interest factor variables (with "none" as the default level), and then create a new variable called "Correct" that notes whether the model was correct for a given prediction. The models are random forest, decision tree, neural net, and XGBoost, as defined in the previous detector rot paper.

```{r data-setup, echo=FALSE}
# load training data
detector_rot_bias_data <- read.csv("path/to/filename.csv", header=TRUE)

# uncomment for gender
# category = 'gender'
# level_list = c("none", "male", "female") # update as appropriate for the given category

# uncomment for race
# category = 'ethnicity'

# black vs non-black only
# temp_category = 'ethnicity'
# detector_rot_bias_data$black = with(detector_rot_bias_data, ifelse(detector_rot_bias_data[, temp_category] == 'black', 'black', 'non-black'))
# category = 'black'

# uncomment for student role
category = 'role'
level_list = c("none", "negative", "neutral", "positive") # update as appropriate for the given category

#make student id and categories of interest factor variables
detector_rot_bias_data$student_id<-as.factor(detector_rot_bias_data$student_id)
detector_rot_bias_data[, category] <- tolower(detector_rot_bias_data[, category])
detector_rot_bias_data[, category] = factor(detector_rot_bias_data[, category], levels=level_list) # none is baseline level


# create column to note whether a given observation was predicted correctly or not, for each model
detector_rot_bias_data$rf_correct<- with(detector_rot_bias_data, ifelse(detector_rot_bias_data[, rf_col] == label, 1, 0))
detector_rot_bias_data$dt_correct <- with(detector_rot_bias_data, ifelse(detector_rot_bias_data[, dt_col] == label, 1, 0))
detector_rot_bias_data$nn_correct <- with(detector_rot_bias_data, ifelse(detector_rot_bias_data[, nn_col] == label, 1, 0))
detector_rot_bias_data$xgb_correct <- with(detector_rot_bias_data, ifelse(detector_rot_bias_data[, xgb_col] == label, 1, 0))

head(detector_rot_bias_data)
```


## Basic HLM explaining whether the model was correct for each model
###Analogous to Overall Accuracy Equality.

```{r correct, echo=FALSE}
rf_model_oae <- glmer(rf_correct ~ detector_rot_bias_data[, category] + (1 | student_id), data=detector_rot_bias_data,
                   family=binomial)
summary(rf_model_oae)


dt_model_oae <- glmer(dt_correct ~ detector_rot_bias_data[, category] + (1 | student_id), data=detector_rot_bias_data,
                   family=binomial)
summary(dt_model_oae)

nn_model_oae <- glmer(nn_correct ~ detector_rot_bias_data[, category] + (1 | student_id), data=detector_rot_bias_data,
                   family=binomial)
summary(nn_model_oae)

xgb_model_oae <- glmer(xgb_correct ~ detector_rot_bias_data[, category] + (1 | student_id), data=detector_rot_bias_data,
                    family=binomial)
summary(xgb_model_oae)
```


### Analogous to Statistical Parity
We can try the models again looking at the prediction of positives only and negatives only, rather than whether the model was correct in general. This is analogous to statistical parity. Because the positive and negative models would be equivalent but reversed we can run the model just once.

```{r stat-parity, echo=FALSE}
rf_model_sp <- glmer(detector_rot_bias_data[, rf_col] ~ detector_rot_bias_data[, category] + (1 | student_id), data=detector_rot_bias_data,
                   family=binomial)
summary(rf_model_sp)


dt_model_sp <- glmer(detector_rot_bias_data[, dt_col] ~ detector_rot_bias_data[, category] + (1 | student_id), data=detector_rot_bias_data,
                   family=binomial)
summary(dt_model_sp)

nn_model_sp <- glmer(detector_rot_bias_data[, nn_col] ~ detector_rot_bias_data[, category] + (1 | student_id), data=detector_rot_bias_data,
                   family=binomial)
summary(nn_model_sp)

xgb_model_sp <- glmer(detector_rot_bias_data[, xgb_col] ~ detector_rot_bias_data[, category] + (1 | student_id), data=detector_rot_bias_data,
                    family=binomial)
summary(xgb_model_sp)
```


### Analogous to conditional procedure accuracy (break groups into pos and neg labels)
We can look at conditional procedure accuracy with two HLMs per machine learning model. We first break the datasets into two groups based on the ground truth labels: observations where gaming was occurring and observations were gaming was not occurring.

We then look at likelihood of correct predictions for each demographic group within the subset.
```{r cpa, echo=FALSE}
gaming_true_labels = detector_rot_bias_data[detector_rot_bias_data$label == 1, ]

gaming_false_labels = detector_rot_bias_data[detector_rot_bias_data$label == 0, ]



rf_true_model_cpa <- glmer(rf_correct ~ gaming_true_labels[, category] + (1 | student_id), data=gaming_true_labels,
                            family=binomial)
summary(rf_true_model_cpa)

rf_false_model_cpa <- glmer(rf_correct ~ gaming_false_labels[, category] + (1 | student_id), data=gaming_false_labels,
                            family=binomial)
summary(rf_false_model_cpa)
# -------------------------------

dt_true_model_cpa <- glmer(dt_correct ~ gaming_true_labels[, category] + (1 | student_id), data=gaming_true_labels,
                            family=binomial)
summary(dt_true_model_cpa)

dt_false_model_cpa <- glmer(dt_correct ~ gaming_false_labels[, category] + (1 | student_id), data=gaming_false_labels,
                            family=binomial)
summary(dt_false_model_cpa)

# -------------------------------

nn_true_model_cpa <- glmer(nn_correct ~ gaming_true_labels[, category] + (1 | student_id), data=gaming_true_labels,
                            family=binomial)
summary(nn_true_model_cpa)

nn_false_model_cpa <- glmer(nn_correct ~ gaming_false_labels[, category] + (1 | student_id), data=gaming_false_labels,
                            family=binomial)
summary(nn_false_model_cpa)

# -------------------------------

xgb_true_model_cpa <- glmer(xgb_correct ~ gaming_true_labels[, category] + (1 | student_id), data=gaming_true_labels,
                            family=binomial)
summary(xgb_true_model_cpa)

xgb_false_model_cpa <- glmer(xgb_correct ~ gaming_false_labels[, category] + (1 | student_id), data=gaming_false_labels,
                             family=binomial)
summary(xgb_false_model_cpa)
```


### Analogous to conditional use accuracy (break groups into pos and neg predictions)
We can also look at conditional use accuracy with two HLMs per machine learning model. We first break the datasets into two groups based on the predicted labels: observations where gaming was predicted as occurring and observations were gaming was not predicted.

We then look at likelihood of correct predictions for each demographic group within the subset.
```{r cua, echo=FALSE}
rf_predicted_gaming = detector_rot_bias_data[detector_rot_bias_data[, rf_col] == 1, ]
dt_predicted_gaming = detector_rot_bias_data[detector_rot_bias_data[, dt_col] == 1, ]
nn_predicted_gaming = detector_rot_bias_data[detector_rot_bias_data[, nn_col] == 1, ]
xgb_predicted_gaming = detector_rot_bias_data[detector_rot_bias_data[, xgb_col] == 1, ]

rf_predicted_not_gaming = detector_rot_bias_data[detector_rot_bias_data[, rf_col] == 0, ]
dt_predicted_not_gaming = detector_rot_bias_data[detector_rot_bias_data[, dt_col] == 0, ]
nn_predicted_not_gaming = detector_rot_bias_data[detector_rot_bias_data[, nn_col] == 0, ]
xgb_predicted_not_gaming = detector_rot_bias_data[detector_rot_bias_data[, xgb_col] == 0, ]

rf_true_model_cua <- glmer(rf_correct ~ rf_predicted_gaming[, category] + (1 | student_id), data=rf_predicted_gaming,
                            family=binomial)
summary(rf_true_model_cua)

# rf_false_model_cua <- glmer(rf_correct ~ rf_predicted_not_gaming[, category] + (1 | student_id), data=rf_predicted_not_gaming,
#                              family=binomial)
# summary(rf_false_model_cua)
# -------------------------------

dt_true_model_cua <- glmer(dt_correct ~ dt_predicted_gaming[, category] + (1 | student_id), data=dt_predicted_gaming,
                            family=binomial)
summary(dt_true_model_cua)

dt_false_model_cua <- glmer(dt_correct ~ dt_predicted_not_gaming[, category] + (1 | student_id), data=dt_predicted_not_gaming,
                             family=binomial)
summary(dt_false_model_cua)

# -------------------------------
nn_true_model_cua <- glmer(nn_correct ~ nn_predicted_gaming[, category] + (1 | student_id), data=nn_predicted_gaming,
                            family=binomial)
summary(nn_true_model_cua)

nn_false_model_cua <- glmer(nn_correct ~ nn_predicted_not_gaming[, category] + (1 | student_id), data=nn_predicted_not_gaming,
                             family=binomial)
summary(nn_false_model_cua)

# -------------------------------
xgb_true_model_cua <- glmer(xgb_correct ~ xgb_predicted_gaming[, category] + (1 | student_id), data=xgb_predicted_gaming,
                             family=binomial)
summary(xgb_true_model_cua)

xgb_false_model_cua <- glmer(xgb_correct ~ xgb_predicted_not_gaming[, category] + (1 | student_id), data=xgb_predicted_not_gaming,
                              family=binomial)
summary(xgb_false_model_cua)

```